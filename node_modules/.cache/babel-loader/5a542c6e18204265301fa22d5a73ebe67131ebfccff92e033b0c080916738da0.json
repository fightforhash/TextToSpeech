{"ast":null,"code":"var _s = $RefreshSig$();\nimport { useState, useEffect } from \"react\";\nlet recognition = null;\nif (\"webkitSpeechRecognition\" in window) {\n  recognition = new webkitSpeechRecognition();\n  recognition.continuous = true;\n  recognition.lang = \"en-US\";\n}\nconst useSpeechRecognition = () => {\n  _s();\n  const [text, setText] = useState(\"\");\n  const [isListening, setIsListening] = useState(false);\n  useEffect(() => {\n    if (!recognition) return;\n    recognition.onresult = event => {\n      console.log(\"onresult event: \", event);\n      setText(event.results[0][0].transcript);\n      recognition.stop();\n      setIsListening(false);\n    };\n  }, []);\n  const startListening = () => {\n    setText('');\n    setIsListening(true);\n    recognition.start();\n  };\n  const stopListening = () => {\n    setIsListening(false);\n    recognition.stop();\n  };\n  return {\n    text,\n    isListening,\n    startListening,\n    stopListening,\n    hasRecognitionSupport: !!recognition\n  };\n};\n_s(useSpeechRecognition, \"YOMFEUt+oC72LKIeNX1WCieAo1w=\");\nexport default useSpeechRecognition;","map":{"version":3,"names":["useState","useEffect","recognition","window","webkitSpeechRecognition","continuous","lang","useSpeechRecognition","_s","text","setText","isListening","setIsListening","onresult","event","console","log","results","transcript","stop","startListening","start","stopListening","hasRecognitionSupport"],"sources":["/Users/mac/Library/Mobile Documents/com~apple~CloudDocs/Washington/TextToSpeech/src/hooks/useSpeechRecognitionHook.ts"],"sourcesContent":["import { useState, useEffect } from \"react\";\n\nlet recognition: any = null;\nif (\"webkitSpeechRecognition\" in window){\n    recognition = new webkitSpeechRecognition(); \n    recognition.continuous = true;\n    recognition.lang = \"en-US\";\n} \n\nconst useSpeechRecognition = () => {\n    const [text, setText] = useState(\"\");        \n    const [isListening, setIsListening] = useState(false);\n\n    useEffect(() => {\n        if (!recognition) return;\n        recognition.onresult = (event: SpeechRecognitionEvent ) => {\n            console.log(\"onresult event: \", event);\n            setText(event.results[0][0].transcript)\n            recognition.stop();\n            setIsListening(false);\n        };\n    }, []);\n\n    const startListening = () => {\n        setText('')\n        setIsListening(true)\n        recognition.start()\n    };\n\n    const stopListening = () => {\n        setIsListening(false)\n        recognition.stop()\n    };\n\n    return {\n        text, \n        isListening,\n        startListening,\n        stopListening,\n        hasRecognitionSupport: !!recognition,\n    };\n};\n\nexport default useSpeechRecognition;\n\n"],"mappings":";AAAA,SAASA,QAAQ,EAAEC,SAAS,QAAQ,OAAO;AAE3C,IAAIC,WAAgB,GAAG,IAAI;AAC3B,IAAI,yBAAyB,IAAIC,MAAM,EAAC;EACpCD,WAAW,GAAG,IAAIE,uBAAuB,CAAC,CAAC;EAC3CF,WAAW,CAACG,UAAU,GAAG,IAAI;EAC7BH,WAAW,CAACI,IAAI,GAAG,OAAO;AAC9B;AAEA,MAAMC,oBAAoB,GAAGA,CAAA,KAAM;EAAAC,EAAA;EAC/B,MAAM,CAACC,IAAI,EAAEC,OAAO,CAAC,GAAGV,QAAQ,CAAC,EAAE,CAAC;EACpC,MAAM,CAACW,WAAW,EAAEC,cAAc,CAAC,GAAGZ,QAAQ,CAAC,KAAK,CAAC;EAErDC,SAAS,CAAC,MAAM;IACZ,IAAI,CAACC,WAAW,EAAE;IAClBA,WAAW,CAACW,QAAQ,GAAIC,KAA6B,IAAM;MACvDC,OAAO,CAACC,GAAG,CAAC,kBAAkB,EAAEF,KAAK,CAAC;MACtCJ,OAAO,CAACI,KAAK,CAACG,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,UAAU,CAAC;MACvChB,WAAW,CAACiB,IAAI,CAAC,CAAC;MAClBP,cAAc,CAAC,KAAK,CAAC;IACzB,CAAC;EACL,CAAC,EAAE,EAAE,CAAC;EAEN,MAAMQ,cAAc,GAAGA,CAAA,KAAM;IACzBV,OAAO,CAAC,EAAE,CAAC;IACXE,cAAc,CAAC,IAAI,CAAC;IACpBV,WAAW,CAACmB,KAAK,CAAC,CAAC;EACvB,CAAC;EAED,MAAMC,aAAa,GAAGA,CAAA,KAAM;IACxBV,cAAc,CAAC,KAAK,CAAC;IACrBV,WAAW,CAACiB,IAAI,CAAC,CAAC;EACtB,CAAC;EAED,OAAO;IACHV,IAAI;IACJE,WAAW;IACXS,cAAc;IACdE,aAAa;IACbC,qBAAqB,EAAE,CAAC,CAACrB;EAC7B,CAAC;AACL,CAAC;AAACM,EAAA,CAhCID,oBAAoB;AAkC1B,eAAeA,oBAAoB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}